{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c3ead2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#install dependencies\n",
    "!pip install torch transformers sentence-transformers faiss-cpu accelerate pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149180f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#GPU\n",
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021183fe",
   "metadata": {},
   "source": [
    "# 1. DATA PREPROCESSING + CLAUSE-AWARE CHUNKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2839f425",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pdfplumber\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 1. PDF EXTRACTION + HEADER REMOVAL\n",
    "# ==============================\n",
    "\n",
    "def extract_pdf_text(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from PDF while removing repeated headers and page numbers.\n",
    "    \"\"\"\n",
    "\n",
    "    full_text = \"\"\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            lines = text.split(\"\\n\")\n",
    "            cleaned_lines = []\n",
    "\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "\n",
    "                # ---- REMOVE REPEATED HEADER (CUSTOMISE IF NEEDED) ----\n",
    "                if \"Regulations on the Use of the UTM Resource Centre\" in line:\n",
    "                    continue\n",
    "\n",
    "                if \"July 2011\" in line or \"V 1.3\" in line:\n",
    "                    continue\n",
    "\n",
    "                # ---- REMOVE STANDALONE PAGE NUMBERS ----\n",
    "                if re.fullmatch(r\"\\d+\", line):\n",
    "                    continue\n",
    "\n",
    "                cleaned_lines.append(line)\n",
    "\n",
    "            page_text = \"\\n\".join(cleaned_lines)\n",
    "            full_text += page_text + \"\\n\"\n",
    "\n",
    "    return full_text\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 2. TEXT CLEANING (YOUR ORIGINAL FUNCTION - IMPROVED)\n",
    "# ==============================\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove formatting artefacts but preserve clause numbering.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove excessive blank lines\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "\n",
    "    # Remove excessive spaces (but keep single spaces)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "    # Ensure regulation numbers stay on new line (e.g., \"1.\", \"2.\")\n",
    "    text = re.sub(r'\\s*(\\d+\\.)\\s*', r'\\n\\1 ', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 3. CLAUSE-AWARE CHUNKING (UNCHANGED LOGIC)\n",
    "# ==============================\n",
    "\n",
    "def clause_aware_chunking(text: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Chunk by Regulation and Clause level only.\n",
    "    Sub-clauses (a), (b), (c) remain inside their parent clause.\n",
    "    \"\"\"\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    # --- Split by Regulation ---\n",
    "    regulation_pattern = r'(?=\\n\\s*\\d+\\.\\s)'\n",
    "    regulations = re.split(regulation_pattern, text)\n",
    "\n",
    "    for reg in regulations:\n",
    "        reg = reg.strip()\n",
    "        if not reg:\n",
    "            continue\n",
    "\n",
    "        reg_match = re.match(r'(\\d+)\\.', reg)\n",
    "        if not reg_match:\n",
    "            continue\n",
    "\n",
    "        reg_number = reg_match.group(1)\n",
    "\n",
    "        # Remove regulation number from text body\n",
    "        reg_body = re.sub(r'^\\d+\\.\\s*', '', reg).strip()\n",
    "\n",
    "        # --- Split by Clause Level (i), (ii), (iii) ---\n",
    "        clause_pattern = r'(?=\\(\\s*[ivx]+\\s*\\))'\n",
    "        clauses = re.split(clause_pattern, reg_body, flags=re.IGNORECASE)\n",
    "\n",
    "        for clause in clauses:\n",
    "            clause = clause.strip()\n",
    "            if not clause:\n",
    "                continue\n",
    "\n",
    "            clause_match = re.match(r'\\(\\s*([ivx]+)\\s*\\)', clause, flags=re.IGNORECASE)\n",
    "            clause_id = clause_match.group(1) if clause_match else \"main\"\n",
    "\n",
    "            # Keep full clause including sub-clauses\n",
    "            clause_text = clause.strip()\n",
    "\n",
    "            if len(clause_text) > 50:\n",
    "                chunks.append({\n",
    "                    \"regulation\": reg_number,\n",
    "                    \"clause\": clause_id,\n",
    "                    \"text\": clause_text\n",
    "                })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 4. COMPLETE PIPELINE\n",
    "# ==============================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    pdf_path = \"/content/drive/MyDrive/RAG-dataset/RCentre.pdf\"  # Your PDF file\n",
    "\n",
    "    raw_text = extract_pdf_text(pdf_path)\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "\n",
    "    chunks = clause_aware_chunking(cleaned_text)\n",
    "\n",
    "    print(f\"Total chunks created: {len(chunks)}\")\n",
    "    print(chunks)\n",
    "\n",
    "    # Optional: Save cleaned text\n",
    "    with open(\"RCentre_cleaned.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cleaned_text)\n",
    "\n",
    "    # Optional: Save chunks\n",
    "    import json\n",
    "    with open(\"RCentre_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a24cb09",
   "metadata": {},
   "source": [
    "# 2. EMBEDDING + VECTOR DATABASE (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c3d8f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def create_vector_index(chunks):\n",
    "    texts = [chunk[\"text\"] for chunk in chunks]\n",
    "\n",
    "    embeddings = embedding_model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    return index, embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f11671",
   "metadata": {},
   "source": [
    "# 3.RETRIEVAL FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc18b66",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve(query, index, chunks, top_k=3):\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    retrieved_chunks = []\n",
    "    for idx in indices[0]:\n",
    "        retrieved_chunks.append(chunks[idx])\n",
    "\n",
    "    return retrieved_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f71c9fb",
   "metadata": {},
   "source": [
    "# 4.GENERATIVE MODEL (RAG GENERATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6213253d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#Load model function\n",
    "def load_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        #load_in_4bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98374590",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def generate_answer(tokenizer, model, query, retrieved_chunks):\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Regulation {c['regulation']}({c['clause']}): {c['text']}\"\n",
    "        for c in retrieved_chunks\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a regulatory assistant for the UTM Resource Centre.\n",
    "Answer strictly using the provided regulations.\n",
    "Cite the regulation number.\n",
    "\n",
    "Regulations:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cb2af5",
   "metadata": {},
   "source": [
    "# 5. Evaluation part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb0ca2c",
   "metadata": {},
   "source": [
    "Definition of the golden dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ba693",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Definition of the golden dataset\n",
    "golden_dataset = [\n",
    "     {\n",
    "        \"id\": \"Q1\",\n",
    "        \"question\": \"What happens if a user fails to return materials by the due date?\",\n",
    "        \"expected_regulation\": \"7\",\n",
    "        \"expected_clause\": \"x\",\n",
    "        \"expected_keywords\": [\"penalty\", \"late\", \"return\"]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7620249",
   "metadata": {},
   "source": [
    "Helper: Extract Citation from Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8611601",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#extract_citaction function\n",
    "import re\n",
    "\n",
    "def extract_citation(answer_text):\n",
    "    \"\"\"\n",
    "    Extract regulation and clause from model output.\n",
    "    Example expected format: Regulation 7(x)\n",
    "    \"\"\"\n",
    "    match = re.search(r'Regulation\\s+(\\d+)\\s*\\(?([ivx]+)?\\)?', answer_text, re.IGNORECASE)\n",
    "\n",
    "    if match:\n",
    "        regulation = match.group(1)\n",
    "        clause = match.group(2)\n",
    "        return regulation, clause\n",
    "\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72cd87f",
   "metadata": {},
   "source": [
    "Definition of the metrics:\n",
    "\n",
    "\n",
    "*   Faithfullness\n",
    "*   Citation accuracy\n",
    "*   Answer relevance\n",
    "*   Hallucination percentage\n",
    "*   Overall score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02923c12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#faithfullness check function\n",
    "def check_faithfulness(answer, retrieved_chunks):\n",
    "    \"\"\"\n",
    "    Checks if answer content is grounded in retrieved chunks.\n",
    "    \"\"\"\n",
    "    combined_context = \" \".join([c[\"text\"] for c in retrieved_chunks])\n",
    "\n",
    "    if answer.lower() in combined_context.lower():\n",
    "        return 2  # Fully supported\n",
    "\n",
    "    # Partial overlap\n",
    "    overlap_count = sum(1 for word in answer.split() if word.lower() in combined_context.lower())\n",
    "\n",
    "    if overlap_count > len(answer.split()) * 0.5:\n",
    "        return 1  # Mostly supported\n",
    "\n",
    "    return 0  # Hallucinated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdcd6a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#citation accuracy check\n",
    "def normalize_clause(clause):\n",
    "    if clause is None:\n",
    "        return \"main\"\n",
    "    clause = clause.lower().strip()\n",
    "    clause = re.sub(r\"\\s+\", \"\", clause)  # remove spaces\n",
    "    return clause\n",
    "\n",
    "def check_citation(answer, expected_regulation, expected_clause):\n",
    "\n",
    "    pred_reg, pred_clause = extract_citation(answer)\n",
    "\n",
    "    if pred_reg is None:\n",
    "        return 0\n",
    "\n",
    "    pred_clause = normalize_clause(pred_clause)\n",
    "    expected_clause = normalize_clause(expected_clause)\n",
    "\n",
    "    if pred_reg == expected_regulation and pred_clause == expected_clause:\n",
    "        return 2  # exact match\n",
    "\n",
    "    elif pred_reg == expected_regulation:\n",
    "        return 1  # regulation correct, clause wrong\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1ac2b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#relevance scoring\n",
    "def check_relevance(answer, expected_keywords):\n",
    "\n",
    "    answer_lower = answer.lower()\n",
    "\n",
    "    if not expected_keywords:\n",
    "        return 0\n",
    "\n",
    "    matches = sum(1 for kw in expected_keywords if kw.lower() in answer_lower)\n",
    "\n",
    "    coverage_ratio = matches / len(expected_keywords)\n",
    "\n",
    "    # Require at least 50% keyword coverage\n",
    "    if coverage_ratio >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd169d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#hallucination detection\n",
    "def detect_hallucination(faithfulness_score):\n",
    "    return 1 if faithfulness_score == 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456aa226",
   "metadata": {},
   "source": [
    "Full pipeline function for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc07d39d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Full evaluation pipeline\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def evaluate_model(model_name):\n",
    "\n",
    "    tokenizer, model = load_model(model_name)\n",
    "\n",
    "    total_faithfulness = 0\n",
    "    total_citation = 0\n",
    "    total_relevance = 0\n",
    "    total_hallucination = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for item in golden_dataset:\n",
    "\n",
    "        question = item[\"question\"]\n",
    "\n",
    "        # Retrieve\n",
    "        retrieved = retrieve(question, index, chunks, top_k=3)\n",
    "\n",
    "        # Generate\n",
    "        answer = generate_answer(tokenizer, model, question, retrieved)\n",
    "\n",
    "        # Score\n",
    "        faith = check_faithfulness(answer, retrieved)\n",
    "        cite = check_citation(answer, item[\"expected_regulation\"], item[\"expected_clause\"])\n",
    "        rel = check_relevance(answer, item[\"expected_keywords\"])\n",
    "        hall = detect_hallucination(faith)\n",
    "\n",
    "        total_faithfulness += faith\n",
    "        total_citation += cite\n",
    "        total_relevance += rel\n",
    "        total_hallucination += hall\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    num_questions = len(golden_dataset)\n",
    "    total_possible = num_questions * 5  # (2 + 2 + 1)\n",
    "\n",
    "    overall_score = ((total_faithfulness + total_citation + total_relevance) / total_possible) * 100\n",
    "    hallucination_rate = (total_hallucination / num_questions) * 100\n",
    "    avg_time = (end_time - start_time) / num_questions\n",
    "\n",
    "    results = {\n",
    "        \"Faithfulness %\": (total_faithfulness / (num_questions * 2)) * 100,\n",
    "        \"Citation %\": (total_citation / (num_questions * 2)) * 100,\n",
    "        \"Relevance %\": (total_relevance / num_questions) * 100,\n",
    "        \"Hallucination Rate %\": hallucination_rate,\n",
    "        \"Overall %\": overall_score,\n",
    "        \"Avg Response Time (s)\": avg_time\n",
    "    }\n",
    "\n",
    "    # Free memory\n",
    "    del model\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f2db4",
   "metadata": {},
   "source": [
    "# 6. FULL PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5120bceb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load your regulation text\n",
    "pdf_path = \"/content/drive/MyDrive/RAG-dataset/RCentre.pdf\"  # Your PDF file\n",
    "\n",
    "raw_text = extract_pdf_text(pdf_path)\n",
    "cleaned_text = clean_text(raw_text)\n",
    "\n",
    "chunks = clause_aware_chunking(cleaned_text)\n",
    "\n",
    "index, embeddings = create_vector_index(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6fa0b2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Pipeline execution for three models\n",
    "results_qwen = evaluate_model(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "print(results_qwen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c8adc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "results_mistral = evaluate_model(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "print(results_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d595f11",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "results_phi = evaluate_model(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "print(results_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e3402",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "results_yi = evaluate_model(\"01-ai/Yi-6B-Chat\")\n",
    "print(results_yi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65734d3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "results_deepseek = evaluate_model(\"deepseek-ai/deepseek-llm-7b-chat\")\n",
    "print(results_deepseek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d7d75a",
   "metadata": {},
   "source": [
    "# 6. SIMPLE CHATBOT (Gradio UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f0a223",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#select current model\n",
    "current_model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer, model = load_model(current_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d938ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Function chatbot_response\n",
    "def chatbot_response(user_query, show_sources):\n",
    "\n",
    "    # Retrieve top 3 clauses\n",
    "    retrieved = retrieve(user_query, index, chunks, top_k=3)\n",
    "\n",
    "    # Generate answer\n",
    "    answer = generate_answer(tokenizer, model, user_query, retrieved)\n",
    "\n",
    "    if show_sources:\n",
    "        sources = \"\\n\\n--- Retrieved Clauses ---\\n\"\n",
    "        for c in retrieved:\n",
    "            sources += f\"\\nRegulation {c['regulation']}({c['clause']}):\\n{c['text']}\\n\"\n",
    "        return answer + sources\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac5aea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Function for loading the selecting model\n",
    "def load_selected_model(model_name):\n",
    "    global tokenizer, model, current_model_name\n",
    "\n",
    "    if model_name != current_model_name:\n",
    "        del model\n",
    "        del tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        tokenizer, model = load_model(model_name)\n",
    "        current_model_name = model_name\n",
    "\n",
    "    return f\"{model_name} loaded successfully.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c63c96",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "custom_css = \"\"\"\n",
    "#container {\n",
    "    max-width: 900px;\n",
    "    margin: auto;\n",
    "}\n",
    ".header {\n",
    "    text-align: center;\n",
    "    padding: 10px;\n",
    "}\n",
    ".footer {\n",
    "    text-align: center;\n",
    "    font-size: 12px;\n",
    "    color: gray;\n",
    "    margin-top: 20px;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "model_loaded_flag = False  # Global control\n",
    "\n",
    "with gr.Blocks(css=custom_css, theme=gr.themes.Soft(), title=\"UTM Regulation RAG Assistant\") as demo:\n",
    "\n",
    "    with gr.Column(elem_id=\"container\"):\n",
    "\n",
    "        # Header\n",
    "        gr.Markdown(\"\"\"\n",
    "        <div class=\"header\">\n",
    "            <h1>üìö UTM Resource Centre Regulation Assistant</h1>\n",
    "            <p>Retrieval-Augmented Generation (RAG) for Regulatory Question Answering</p>\n",
    "        </div>\n",
    "        \"\"\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Model Configuration\n",
    "        # -------------------------\n",
    "        with gr.Group():\n",
    "            gr.Markdown(\"### ‚öôÔ∏è Model Configuration\")\n",
    "\n",
    "            model_selector = gr.Dropdown(\n",
    "                choices=[\n",
    "                    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "                    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "                    \"microsoft/Phi-3.5-mini-instruct\",\n",
    "                    \"01-ai/Yi-6B-Chat\",\n",
    "                    \"deepseek-ai/deepseek-llm-7b-chat\"\n",
    "                ],\n",
    "                value=current_model_name,\n",
    "                label=\"Select Large Language Model\"\n",
    "            )\n",
    "\n",
    "            load_button = gr.Button(\"üöÄ Load Selected Model\", variant=\"primary\")\n",
    "\n",
    "            model_status = gr.Textbox(\n",
    "                label=\"Model Status\",\n",
    "                interactive=False,\n",
    "                placeholder=\"No model loaded.\"\n",
    "            )\n",
    "\n",
    "        # -------------------------\n",
    "        # Chat Section\n",
    "        # -------------------------\n",
    "        gr.Markdown(\"### üí¨ Ask About the Regulations\")\n",
    "\n",
    "        chatbot = gr.Chatbot(\n",
    "            label=\"Conversation\",\n",
    "            height=400,\n",
    "            bubble_full_width=False\n",
    "        )\n",
    "\n",
    "        with gr.Row():\n",
    "            msg = gr.Textbox(\n",
    "                label=\"Your Question\",\n",
    "                placeholder=\"Load a model first...\",\n",
    "                scale=4,\n",
    "                interactive=False   # üîí initially disabled\n",
    "            )\n",
    "\n",
    "            send_button = gr.Button(\n",
    "                \"Send\",\n",
    "                variant=\"secondary\",\n",
    "                scale=1,\n",
    "                interactive=False   # üîí initially disabled\n",
    "            )\n",
    "\n",
    "        with gr.Row():\n",
    "            show_sources = gr.Checkbox(label=\"Show Retrieved Clauses\")\n",
    "            clear = gr.Button(\"üóë Clear Chat\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Backend Functions\n",
    "        # -------------------------\n",
    "\n",
    "        def load_model_ui(selected_model):\n",
    "            global model_loaded_flag\n",
    "            try:\n",
    "                status_message = load_selected_model(selected_model)\n",
    "                model_loaded_flag = True\n",
    "                return (\n",
    "                    f\"‚úÖ {status_message}\",\n",
    "                    gr.update(interactive=True, placeholder=\"Type your question here...\"),\n",
    "                    gr.update(interactive=True)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                model_loaded_flag = False\n",
    "                return (\n",
    "                    f\"‚ùå Error loading model: {str(e)}\",\n",
    "                    gr.update(interactive=False),\n",
    "                    gr.update(interactive=False)\n",
    "                )\n",
    "\n",
    "        def respond(message, chat_history, show_sources):\n",
    "            if not model_loaded_flag:\n",
    "                return \"\", chat_history\n",
    "\n",
    "            if not message.strip():\n",
    "                return \"\", chat_history\n",
    "\n",
    "            response = chatbot_response(message, show_sources)\n",
    "            chat_history.append((message, response))\n",
    "            return \"\", chat_history\n",
    "\n",
    "        # -------------------------\n",
    "        # Bind Events\n",
    "        # -------------------------\n",
    "\n",
    "        load_button.click(\n",
    "            load_model_ui,\n",
    "            inputs=model_selector,\n",
    "            outputs=[model_status, msg, send_button]\n",
    "        )\n",
    "\n",
    "        msg.submit(\n",
    "            respond,\n",
    "            [msg, chatbot, show_sources],\n",
    "            [msg, chatbot]\n",
    "        )\n",
    "\n",
    "        send_button.click(\n",
    "            respond,\n",
    "            [msg, chatbot, show_sources],\n",
    "            [msg, chatbot]\n",
    "        )\n",
    "\n",
    "        clear.click(\n",
    "            lambda: [],\n",
    "            None,\n",
    "            chatbot,\n",
    "            queue=False\n",
    "        )\n",
    "\n",
    "        # Footer\n",
    "        gr.Markdown(\"\"\"\n",
    "        <div class=\"footer\">\n",
    "        MSc Coursework Project ‚Äî Regulatory RAG System Evaluation (Qwen2.5 | Mistral | Phi-3.5)\n",
    "        </div>\n",
    "        \"\"\")\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
