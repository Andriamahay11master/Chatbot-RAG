{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c3ead2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#install dependencies\n",
    "!pip install torch transformers sentence-transformers faiss-cpu accelerate pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149180f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#GPU\n",
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021183fe",
   "metadata": {},
   "source": [
    "# 1. DATA PREPROCESSING + CLAUSE-AWARE CHUNKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2839f425",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove formatting artefacts but preserve clause numbering.\n",
    "    \"\"\"\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove page numbers if any (customise if needed)\n",
    "    text = re.sub(r'\\b\\d+\\s*DEC\\s*2025\\b', '', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def clause_aware_chunking(text: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Split document by Regulation and clause markers.\n",
    "    Returns structured chunks with metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    # Split by main Regulation numbers (1. Scope, 2. Entitlement, etc.)\n",
    "    regulation_pattern = r'(?=\\n?\\s*\\d+\\.\\s)'\n",
    "    regulations = re.split(regulation_pattern, text)\n",
    "\n",
    "    for reg in regulations:\n",
    "        reg = reg.strip()\n",
    "        if not reg:\n",
    "            continue\n",
    "\n",
    "        # Extract regulation number\n",
    "        reg_match = re.match(r'(\\d+)\\.', reg)\n",
    "        if not reg_match:\n",
    "            continue\n",
    "\n",
    "        reg_number = reg_match.group(1)\n",
    "\n",
    "        # Split clauses (i), (ii), (iii)\n",
    "        clause_pattern = r'(?=\\(\\w+\\))'\n",
    "        clauses = re.split(clause_pattern, reg)\n",
    "\n",
    "        for clause in clauses:\n",
    "            clause = clause.strip()\n",
    "            if len(clause) < 50:  # ignore tiny fragments\n",
    "                continue\n",
    "\n",
    "            clause_id_match = re.match(r'\\((\\w+)\\)', clause)\n",
    "            clause_id = clause_id_match.group(1) if clause_id_match else \"main\"\n",
    "\n",
    "            chunks.append({\n",
    "                \"regulation\": reg_number,\n",
    "                \"clause\": clause_id,\n",
    "                \"text\": clause.strip()\n",
    "            })\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a24cb09",
   "metadata": {},
   "source": [
    "# 2. EMBEDDING + VECTOR DATABASE (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c3d8f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def create_vector_index(chunks):\n",
    "    texts = [chunk[\"text\"] for chunk in chunks]\n",
    "\n",
    "    embeddings = embedding_model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    return index, embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f11671",
   "metadata": {},
   "source": [
    "# 3.RETRIEVAL FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc18b66",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve(query, index, chunks, top_k=3):\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    retrieved_chunks = []\n",
    "    for idx in indices[0]:\n",
    "        retrieved_chunks.append(chunks[idx])\n",
    "\n",
    "    return retrieved_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f71c9fb",
   "metadata": {},
   "source": [
    "# 4.GENERATIVE MODEL (RAG GENERATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98374590",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "def generate_answer(query, retrieved_chunks):\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Regulation {c['regulation']}({c['clause']}): {c['text']}\"\n",
    "        for c in retrieved_chunks\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a regulatory assistant for the UTM Resource Centre.\n",
    "Answer strictly using the provided regulations.\n",
    "Cite the regulation number.\n",
    "\n",
    "Regulations:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f2db4",
   "metadata": {},
   "source": [
    "# 5. FULL PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5120bceb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load your regulation text\n",
    "with open(\"RCentre.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "cleaned_text = clean_text(raw_text)\n",
    "chunks = clause_aware_chunking(cleaned_text)\n",
    "\n",
    "index, embeddings = create_vector_index(chunks)\n",
    "\n",
    "# Example query\n",
    "query = \"What happens if a member fails to return materials by the due date?\"\n",
    "\n",
    "retrieved = retrieve(query, index, chunks, top_k=3)\n",
    "answer = generate_answer(query, retrieved)\n",
    "\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
